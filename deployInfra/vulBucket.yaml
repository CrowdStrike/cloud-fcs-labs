AWSTemplateFormatVersion: 2010-09-09
Parameters:
  EnvAlias:
    Type: String
    Default: ''
  EnvHash:
    Type: String
    Default: ''
  # PermissionsBoundary:
  #   Type: String
  #   Default: ''
  S3Bucket:
    Type: String
    Default: ''
  S3Prefix:
    Type: String
    Default: ''
  SourceObjects:
    Type: CommaDelimitedList
    Default: "confidential-data.txt"
# Conditions:
  # PermissionsBoundary: !Not [ !Equals [ !Ref PermissionsBoundary, '' ] ]
Resources:
  ConfidentialLoggingBucketParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Description: set logging S3bucket parameter for stack deletion cleanup
      Name: !Sub 'psLoggingBucket-${EnvHash}'
      Type: String
      Value: !Ref ConfidentialLoggingBucket  
      
  ConfidentialBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      AccessControl: Private
      LoggingConfiguration:
        DestinationBucketName: !Ref ConfidentialLoggingBucket
        LogFilePrefix: testing-logs
  ConfidentialLoggingBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      AccessControl: LogDeliveryWrite
      OwnershipControls:
        Rules:
          - ObjectOwnership: BucketOwnerPreferred
  CopyRole:
    Type: AWS::IAM::Role
    Properties:
      Path: /
      # PermissionsBoundary:
      #   Fn::If:
      #     - PermissionsBoundary
      #     - !Sub 'arn:aws:iam::${AWS::AccountId}:policy/${PermissionsBoundary}'
          # - Ref: AWS::NoValue
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Sub 'arn:${AWS::Partition}:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:
        - PolicyName: ConfigPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Sid: S3Get
                Effect: Allow
                Action:
                  - s3:GetObject
                Resource: !Sub 'arn:${AWS::Partition}:s3:::${S3Bucket}/${S3Prefix}/*'
                #  - SourceS3Bucket: !Ref S3Bucket
              - Sid: S3Put
                Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:DeleteObject
                Resource: !Sub 'arn:${AWS::Partition}:s3:::${ConfidentialBucket}/*'
                #  - DestS3Bucket: !Ref ConfidentialBucket
  CopyFiles:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt CopyFilesFunction.Arn
      DestRegion: !Ref "AWS::Region"
      DestBucket: !Ref ConfidentialBucket
      SourceBucket: !Join ['/', [!Ref S3Bucket, !Ref S3Prefix]]
      Objects: !Ref SourceObjects
  CopyFilesFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Copies objects from a source S3 bucket to a destination
      Handler: index.handler
      Runtime: python3.8
      Role: !GetAtt CopyRole.Arn
      Timeout: 240
      Code:
        ZipFile: |
          import json
          import logging
          import threading
          import boto3
          import cfnresponse
          def copy_objects(source_bucket, dest_bucket, objects):
              s3 = boto3.client('s3')
              for o in objects:
                  copy_source = {
                      'Bucket': source_bucket,
                      'Key': o
                  }
                  s3.copy_object(CopySource=copy_source, Bucket=dest_bucket, Key=o)
          def delete_objects(bucket, objects):
              s3 = boto3.client('s3')
              objects = {'Objects': [{'Key': o} for o in objects]}
              s3.delete_objects(Bucket=bucket, Delete=objects)
          def timeout(event, context):
              logging.error('Execution is about to time out, sending failure response to CloudFormation')
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None)
          def handler(event, context):
              # make sure we send a failure to CloudFormation if the function is going to timeout
              timer = threading.Timer((context.get_remaining_time_in_millis() / 1000.00) - 0.5, timeout, args=[event, context])
              timer.start()
              print('Received event: %s' % json.dumps(event))
              status = cfnresponse.SUCCESS
              try:
                  source_bucket = event['ResourceProperties']['SourceBucket']
                  dest_bucket = event['ResourceProperties']['DestBucket']
                  objects = event['ResourceProperties']['Objects']
                  if event['RequestType'] == 'Delete':
                      delete_objects(dest_bucket, objects)
                  else:
                      copy_objects(source_bucket, dest_bucket, objects)
              except Exception as e:
                  logging.error('Exception: %s' % e, exc_info=True)
                  status = cfnresponse.FAILED
              finally:
                  timer.cancel()
                  cfnresponse.send(event, context, status, {}, None)

Outputs:
  BucketName:
    Value: !Ref ConfidentialBucket
    Description: Name of the sample Amazon S3 bucket with a logging configuration.

